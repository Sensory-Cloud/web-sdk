import { EnrollmentType, SDKInitConfig } from "../../src/config";
import { OauthService } from "../../src/services/oauth.service";
import { AudioService } from "../../src/services/audio.service";
import { TokenManager } from "../../src/token-manager/token.manager";
import { Initializer } from "../../src/initializer";
import { AudioEvent, AudioStreamInteractor } from "../../src/interactors/audio-stream.interactor";
import { SecureCredentialStoreExample } from "./secure-credential-store.example";
import { AuthenticateRequest, CreateEnrollmentRequest, GetModelsResponse, ValidateEventRequest } from "../../src/generated/v1/audio/audio_pb";

// Tenant ID granted by Sensory Inc.
const sensoryTenantId: string = 'f6580f3b-dcaf-465b-867e-59fbbb0ab3fc';
// Device enrollment credential configured in sensory cloud server
const credential: string = 'secure-password';
// Globally Unique device ID generated by you to pin the web-sdk to a browser.
const uniqueDeviceId: string = '337ed9ac-4c0f-4cd2-9ecc-51f712e53e92';

const config: SDKInitConfig = {
  fullyQualifiedDomainName: 'your-inference-server.com',
  isSecure: true,
  tenantId: sensoryTenantId,
  enrollmentType: EnrollmentType.sharedSecret,
  credential: credential,
  deviceId: uniqueDeviceId,
  deviceName: 'example-device'
}

// Implement your own secure credential store
const credentialStore = new SecureCredentialStoreExample();
const oauthService = new OauthService(credentialStore);
const tokenManager = new TokenManager(oauthService);

// Initialize the SDK
await Initializer.initializeFromConfig(config, oauthService);

// AudioStreamInteractor is a Sensory implementation that interacts with microphones through the web browser
const audioStreamInteractor = new AudioStreamInteractor();

const audioService = new AudioService(tokenManager, audioStreamInteractor);

const getAudioService = (): AudioService => {
  // Implement your own secure credential store
  const credentialStore = new SecureCredentialStoreExample();
  const oauthService = new OauthService(credentialStore);
  const tokenManager = new TokenManager(oauthService);

  // AudioStreamInteractor is a Sensory implementation that interacts with microphones through the web browser
  const audioStreamInteractor = new AudioStreamInteractor();

  return new AudioService(tokenManager, audioStreamInteractor);
}

const exampleGetAudioModels = (): Promise<GetModelsResponse.AsObject> => {
  const audioService = getAudioService();
  return audioService.getModels();
}

const exampleEnrollWithAudio = async () => {
  // Set basic enrollment information
  const enrollmentDescription = "My Enrollment";
  const userId = "72f286b8-173f-436a-8869-6f7887789ee9";
  const modelName = "wakeword-16kHz-open_sesame.ubm";
  const isLivenessEnabled = false;
  const uploadInterval = 100; // Upload audio every 100ms

  // Begin gRPC enrollment stream
  const enrollmentStream = await audioService.streamEnrollment(
    enrollmentDescription,
    userId,
    modelName,
    isLivenessEnabled);

  // enrollmentId will be populated on the very last message if enrollment
  // is successful.
  let enrollmentId: string|null = null;

  // Handle responses from the server
  enrollmentStream.on('data', (response) => {
    // Response contains information about the enrollment status.
    // * audioEnergy
    // * percentComplete
    // For enrollments with liveness, there are two additional fields that are populated.
    // * modelPrompt - indicates what the user should say in order to proceed with the enrollment.
    // * sectionPercentComplete - indicates the percentage of the current ModelPrompt that has been spoken.
    enrollmentId = response.getEnrollmentid();
  });

  // Start microphone recording
  const audioEvent = await audioStreamInteractor.startCapturing(uploadInterval);

  // Register onData callback for the microphone audio
  const onData = (evt: CustomEvent<AudioEvent>) => {

    // Pack and send bytes to the server
    const request = new CreateEnrollmentRequest();
    request.setAudiocontent(evt.detail.bytes);

    try {
      enrollmentStream.write(request);
    } catch (err) {
      console.error(err);
    }
  }

  // Register callback
  audioEvent.addListener(onData);

  // Handle end of stream. This promise can be used to block until enrollment is complete.
  return new Promise<string>((resolve, reject) => {
    enrollmentStream.on('end', (status) => {
      audioEvent.removeListener(onData);
      audioStreamInteractor.stopCapturing();

      if (status?.code !== 0 || !enrollmentId?.length) {
        reject(`Enrollment failed due to reason: ${status?.details || 'enrollment was not complete'}`);
      } else {
        // Enrollment success!
        // Be sure to save enrollmentId for use later with authentication
        resolve(enrollmentId);
      }
    });
  });
}

const exampleAuthenticateWithAudio = async () => {
  // Get enrollmentId from the prior enrollment or from the getEnrollments call
  // offered by the ManagementService.
  const enrollmentId = "72f286b8-173f-436a-8869-6f7887789ee9";
  const isLivenessEnabled = false;
  const uploadInterval = 100; // Upload audio every 100ms

  // Begin gRPC authentication stream
  // For group authentication (groups of users up to 10) you can use
  // {enrollmentGroupId: enrollmentId} rather than {enrollmentId: enrollmentId}
  const authenticationStream = await audioService.streamAuthentication(
    {enrollmentId: enrollmentId}, isLivenessEnabled);

  // isAuthenticated will be true on the very last message if auth is successful.
  let isAuthenticated = false;

  // Handle responses from the server
  authenticationStream.on('data', (response) => {
    // Response contains information about the authentication audio such as:
    // * audioEnergy
    // For authentications with liveness, there are two additional fields that are populated.
    // * modelPrompt - indicates what the user should say in order to proceed with the authentication.
    // * sectionPercentComplete - indicates the percentage of the current ModelPrompt that has been spoken.
    isAuthenticated = response.getSuccess();
  });

  // Start microphone recording
  const audioEvent = await audioStreamInteractor.startCapturing(uploadInterval);

  // Register onData callback for the microphone audio
  const onData = (evt: CustomEvent<AudioEvent>) => {

    // Pack and send bytes to the server
    const request = new AuthenticateRequest();
    request.setAudiocontent(evt.detail.bytes);

    try {
      authenticationStream.write(request);
    } catch (err) {
      console.error(err);
    }
  }

  // Register callback
  audioEvent.addListener(onData);

  // Handle end of stream. This promise can be used to block until authentication is complete.
  return new Promise<boolean>((resolve, reject) => {
    authenticationStream.on('end', (status) => {
      audioEvent.removeListener(onData);
      audioStreamInteractor.stopCapturing();

      if (status?.code !== 0 || !isAuthenticated) {
        reject(`Authentication failed due to reason: ${status?.details || 'authentication was not successful'}`);
      } else {
        // Authentication success!
        resolve(isAuthenticated);
      }
    });
  });
}

const exampleRecognizeWithAudio = async () => {
  const userId = "72f286b8-173f-436a-8869-6f7887789ee9";
  const modelName = "wakeword-16kHz-open_sesame.ubm";
  const uploadInterval = 100; // Upload audio every 100ms

  // Unlike enrollment and authentication, it is up to you, the implementer
  // to determine when to close the connection the server.
  const eventStream = await audioService.streamEvent(userId, modelName);

  // Handle responses from the server
  eventStream.on('data', (response) => {
    // Response contains information about the audio such as:
    // * audioEnergy

    // Event was recognized
    if (response.getSuccess()) {
      // Response will contain the following if the event was recognized
      // * resultId - indicating the name of the event that was recognized
      // * score - Sensory's confidence in the result

      // As an example, close the connection to the server after obtaining 1 event result.
      // You may also want a timeout to handle if the server never responds with a result.
      eventStream.end();
    }
  });

  // Start microphone recording
  const audioEvent = await audioStreamInteractor.startCapturing(uploadInterval);

  // Register onData callback for the microphone audio
  const onData = (evt: CustomEvent<AudioEvent>) => {

    // Pack and send bytes to the server
    const request = new ValidateEventRequest();
    request.setAudiocontent(evt.detail.bytes);

    try {
      eventStream.write(request);
    } catch (err) {
      console.error(err);
    }
  }

  // Register callback
  audioEvent.addListener(onData);

  // Handle end of stream. This promise can be used to block for as long as you'd like.
  // It's up to you, the SDk implementer to decide when to end this request.
  return new Promise<void>((resolve, reject) => {
    eventStream.on('end', (status) => {
      audioEvent.removeListener(onData);
      audioStreamInteractor.stopCapturing();

      if (status?.code !== 0 ) {
        reject(`Event recognition failed due to reason: ${status?.details || 'no response was returned'}`);
      } else {
        // Event success!
        resolve();
      }
    });
  });
}

const exampleTranscribeWithAudio = async () => {
  const userId = "72f286b8-173f-436a-8869-6f7887789ee9";
  const modelName = "wakeword-16kHz-open_sesame.ubm";
  const uploadInterval = 100; // Upload audio every 100ms

  // Unlike enrollment and authentication, it is up to you, the implementer
  // to determine when to close the connection the server.
  const transcribeStream = await audioService.streamTranscription(userId, modelName);

  // As an example, set a timeout for 10 seconds and then close the stream
  setTimeout(transcribeStream.end, 10000);

  // Handle responses from the server
  transcribeStream.on('data', (response) => {
    // Response contains information about the audio such as:
    // * audioEnergy

    // Transcript contains the current running transcript of the data
    const transcript = response.getTranscript();
  });

  // Start microphone recording
  const audioEvent = await audioStreamInteractor.startCapturing(uploadInterval);

  // Register onData callback for the microphone audio
  const onData = (evt: CustomEvent<AudioEvent>) => {

    // Pack and send bytes to the server
    const request = new ValidateEventRequest();
    request.setAudiocontent(evt.detail.bytes);

    try {
      transcribeStream.write(request);
    } catch (err) {
      console.error(err);
    }
  }

  // Register callback
  audioEvent.addListener(onData);

  // Handle end of stream. This promise can be used to block for as long as you'd like.
  // It's up to you, the SDk implementer to decide when to end this request.
  return new Promise<void>((resolve, reject) => {
    transcribeStream.on('end', (status) => {
      audioEvent.removeListener(onData);
      audioStreamInteractor.stopCapturing();

      if (status?.code !== 0 ) {
        reject(`Transcription failed due to reason: ${status?.details || 'no response was returned'}`);
      } else {
        // Transcription success!
        resolve();
      }
    });
  });
}